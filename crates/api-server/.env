# SecureLLM Bridge Configuration
# Server Configuration
SERVER_HOST=0.0.0.0
SERVER_PORT=8080
SERVER_WORKERS=4

# Database Configuration (local development)
DATABASE_URL=sqlite:/home/kernelcore/arch/securellm-bridge/data/models.db

# Redis Configuration
REDIS_URL=redis://localhost:6379

# Provider Configuration

# DeepSeek
DEEPSEEK_ENABLED=true
DEEPSEEK_API_KEY=your-deepseek-api-key-here
DEEPSEEK_BASE_URL=https://api.deepseek.com

# OpenAI
OPENAI_ENABLED=false
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic (Claude)
ANTHROPIC_ENABLED=false
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1

# Groq
GROQ_ENABLED=false
GROQ_API_KEY=your-groq-api-key-here
GROQ_BASE_URL=https://api.groq.com/openai/v1

# Gemini
GEMINI_ENABLED=false
GEMINI_API_KEY=your-gemini-api-key-here

# NVIDIA
NVIDIA_ENABLED=false
NVIDIA_API_KEY=your-nvidia-api-key-here

# LlamaCpp (Local Inference on Host)
LLAMACPP_ENABLED=true
LLAMACPP_BASE_URL=http://localhost:5001
LLAMACPP_MODEL_NAME=local-model

# Security
API_KEYS=
REQUIRE_AUTH=false

# Telemetry
LOG_LEVEL=info
LOG_DIR=./logs
OTLP_ENDPOINT=
