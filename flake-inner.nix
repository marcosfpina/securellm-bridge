# flake.nix - Sistema completo de Knowledge Extraction + RAG Local
{
  description = "PHANTOM CEREBRO - Knowledge Engine";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs = { self, nixpkgs, flake-utils }:
    flake-utils.lib.eachDefaultSystem (system:
      let
        pkgs = import nixpkgs {
          inherit system;
          config.allowUnfree = true; # pra CUDA se necessÃ¡rio
        };

        # Python environment completo
        pythonEnv = pkgs.python313.withPackages (ps: with ps; [
          # GCP Integration
          google-cloud-discoveryengine
          google-cloud-aiplatform
          google-cloud-storage

          # Embeddings & Vector DB
          sentence-transformers
          chromadb
          faiss-gpu

          # Local LLMs
          transformers
          torch
          accelerate
          bitsandbytes  # quantizaÃ§Ã£o 4-bit

          # Code Analysis
          tree-sitter
          tree-sitter-languages
          pygments

          # Docs parsing
          gitpython
          markdown
          beautifulsoup4

          # Utils
          pydantic
          tqdm
          rich
          typer
        ]);

        # Scripts de anÃ¡lise
        knowledge-extractor = pkgs.writeShellScriptBin "cerebro-extract" ''
          #!/usr/bin/env bash
          set -euo pipefail

          echo "ðŸ§  CEREBRO Knowledge Extraction Pipeline"
          echo "========================================"

          # 1. Clone/update all repos
          echo "ðŸ“¥ Cloning Repos..."
          ${pythonEnv}/bin/python ${./scripts/01_clone_repos.py} \
            --repos-file ${./repos.yaml}

          # 2. Parse & analyze code
          echo "ðŸ”¬ Analyzing Structure..."
          ${pythonEnv}/bin/python ${./scripts/02_analyze_code.py} \
            --output ${./data/analyzed}

          # 3. Generate embeddings (GCP Vertex AI)
          echo "ðŸ”¥ Ingesting to Vertex AI (Credit Burn Mode)..."
          ${pythonEnv}/bin/python ${./scripts/03_ingest_data.py} \
            --jsonl ${./data/analyzed/all_artifacts.jsonl} \
            --datastore-id "phantom-knowledge-base"
          #--provider vertex-ai \
          #--model text-embedding-004 \
          #--output ${./data/embeddings}

          # 4. Index to vector DB
          #${pythonEnv}/bin/python ${./scripts/04_index_vectors.py} \
          #--db-path ${./data/cerebro.db}

          echo "âœ… IngestÃ£o completa! Iterate and Validate Sir"
        '';

        # RAG local server
        cerebro-server = pkgs.writeShellScriptBin "cerebro-serve" ''
          #!/usr/bin/env bash
          set -euo pipefail

          echo "ðŸš€ Starting CEREBRO RAG Server"

          # Start local LLM server
          ${pythonEnv}/bin/python ${./src/server.py} \
            --model-name "mistral-7b-instruct-v0.2" \
            --quantization "4bit" \
            --db-path ${./data/cerebro.db} \
            --port 8000
        '';

        # CLI tool
        cerebro-cli = pkgs.writeShellScriptBin "cerebro" ''
          #!/usr/bin/env bash
          ${pythonEnv}/bin/python ${./src/cli.py} "$@"
        '';

      in {
        packages = {
          default = cerebro-cli;
          extractor = knowledge-extractor;
          server = cerebro-server;
        };

        devShells.default = pkgs.mkShell {
          buildInputs = [
            pythonEnv
            pkgs.git
            pkgs.just
            pkgs.llama-cpp  # pra testar modelos locais
          ] ++ (if pkgs.stdenv.isLinux then [
            pkgs.cudaPackages.cudatoolkit
          ] else []);

          shellHook = ''
            export CEREBRO_DATA_DIR="./data"
            export CEREBRO_CACHE_DIR="./cache"
            export GCP_PROJECT_ID="seu-projeto-gcp"
            export GCP_REGION="us-central1"

            # Load GCP credentials
            if [ -f ~/.config/gcloud/application_default_credentials.json ]; then
              export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/application_default_credentials.json
            fi

            echo "ðŸ§  CEREBRO Development Environment"
            echo "=================================="
            echo ""
            echo "Available commands:"
            echo "  cerebro-extract  - Extract knowledge from repos (uses GCP credits)"
            echo "  cerebro-serve    - Start local RAG server"
            echo "  cerebro query    - Query knowledge base"
            echo ""
            echo "Data directory: $CEREBRO_DATA_DIR"
          '';
        };

        # NixOS module pra rodar como serviÃ§o
        nixosModules.cerebro = { config, lib, pkgs, ... }: {
          options.services.cerebro = {
            enable = lib.mkEnableOption "CEREBRO RAG server";

            port = lib.mkOption {
              type = lib.types.port;
              default = 8000;
              description = "Port for CEREBRO server";
            };

            modelName = lib.mkOption {
              type = lib.types.str;
              default = "mistral-7b-instruct-v0.2";
              description = "Local model to use";
            };

            dbPath = lib.mkOption {
              type = lib.types.path;
              default = "/var/lib/cerebro/cerebro.db";
              description = "Path to knowledge base";
            };
          };

          config = lib.mkIf config.services.cerebro.enable {
            systemd.services.cerebro = {
              description = "CEREBRO RAG Server";
              wantedBy = [ "multi-user.target" ];
              after = [ "network.target" ];

              serviceConfig = {
                Type = "simple";
                ExecStart = "${cerebro-server}/bin/cerebro-serve";
                Restart = "on-failure";
                RestartSec = "10s";

                # Security hardening
                DynamicUser = true;
                StateDirectory = "cerebro";
                CacheDirectory = "cerebro";
                PrivateTmp = true;
                ProtectSystem = "strict";
                ProtectHome = true;
                NoNewPrivileges = true;
              };

              environment = {
                CEREBRO_PORT = toString config.services.cerebro.port;
                CEREBRO_MODEL = config.services.cerebro.modelName;
                CEREBRO_DB = config.services.cerebro.dbPath;
              };
            };

            # Firewall
            networking.firewall.allowedTCPPorts = [ config.services.cerebro.port ];
          };
        };
      }
    );
}
