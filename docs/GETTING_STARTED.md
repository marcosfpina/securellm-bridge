# SecureLLM Bridge - Guia Completo do Projeto

## ğŸ“‹ VisÃ£o Geral

O **SecureLLM Bridge** Ã© uma soluÃ§Ã£o completa e segura para comunicaÃ§Ã£o com modelos de linguagem (LLMs), focada em:

- **SeguranÃ§a por padrÃ£o**: Todas as comunicaÃ§Ãµes sÃ£o criptografadas e validadas
- **Isolamento mÃ¡ximo**: Sandboxing e proteÃ§Ã£o de dados sensÃ­veis
- **Suporte multi-provider**: DeepSeek, OpenAI, Anthropic, Ollama e outros
- **Flexibilidade**: Desktop, CLI, containers e biblioteca Rust

## ğŸ—ï¸ Arquitetura

### Estrutura do Projeto

```
secure-llm-bridge/
â”œâ”€â”€ crates/
â”‚   â”œâ”€â”€ core/           # Biblioteca principal com abstraÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ request.rs  # GestÃ£o de requisiÃ§Ãµes com validaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ response.rs # Processamento de respostas
â”‚   â”‚   â”œâ”€â”€ error.rs    # Sistema de erros tipado
â”‚   â”‚   â””â”€â”€ audit.rs    # Sistema de auditoria
â”‚   â”‚
â”‚   â”œâ”€â”€ security/       # Primitivos de seguranÃ§a
â”‚   â”‚   â”œâ”€â”€ tls.rs      # AutenticaÃ§Ã£o mÃºtua TLS
â”‚   â”‚   â”œâ”€â”€ crypto.rs   # Criptografia AES-256-GCM
â”‚   â”‚   â”œâ”€â”€ secrets.rs  # GestÃ£o segura de secrets
â”‚   â”‚   â””â”€â”€ sandbox.rs  # Isolamento de execuÃ§Ã£o
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/      # ImplementaÃ§Ãµes de providers
â”‚   â”‚   â”œâ”€â”€ deepseek.rs # âœ… Implementado (completo)
â”‚   â”‚   â”œâ”€â”€ openai.rs   # ğŸš§ Placeholder
â”‚   â”‚   â”œâ”€â”€ anthropic.rs# ğŸš§ Placeholder
â”‚   â”‚   â””â”€â”€ ollama.rs   # ğŸš§ Placeholder (local)
â”‚   â”‚
â”‚   â”œâ”€â”€ cli/            # Interface de linha de comando
â”‚   â”œâ”€â”€ desktop/        # ğŸš§ AplicaÃ§Ã£o desktop (futuro)
â”‚   â””â”€â”€ proxy/          # ğŸš§ Servidor proxy HTTP/S (futuro)
â”‚
â”œâ”€â”€ containers/         # Dockerfiles e compose
â”œâ”€â”€ nix/                # ConfiguraÃ§Ã£o NixOS
â”œâ”€â”€ examples/           # Exemplos de uso
â””â”€â”€ docs/               # DocumentaÃ§Ã£o adicional
```

## ğŸš€ Como ComeÃ§ar

### PrÃ©-requisitos

#### MÃ©todo 1: Usando Cargo (Rust)

```bash
# Instalar Rust
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clonar o repositÃ³rio
git clone https://github.com/securellm/bridge.git
cd secure-llm-bridge

# Build
cargo build --release

# Instalar globalmente (opcional)
cargo install --path crates/cli
```

#### MÃ©todo 2: Usando Nix (NixOS/Nix)

```bash
# Com flakes habilitado
nix build
nix run

# Entrar no ambiente de desenvolvimento
nix develop

# Para usuÃ¡rios NixOS - adicione ao configuration.nix:
services.securellm = {
  enable = true;
  configFile = /etc/securellm/config.toml;
};
```

#### MÃ©todo 3: Usando Docker

```bash
# Build da imagem
docker build -t securellm:latest .

# Ou usar Alpine (mais leve)
docker build -f containers/Dockerfile.alpine -t securellm:alpine .

# Executar
docker run --rm securellm:latest --help
```

## ğŸ’¡ Uso BÃ¡sico

### CLI - Linha de Comando

```bash
# Configurar API key
export SECURELLM_API_KEY="your-deepseek-api-key"

# Chat simples
securellm chat \
  --provider deepseek \
  --model deepseek-chat \
  "Explique computaÃ§Ã£o quÃ¢ntica de forma simples"

# Com system prompt
securellm chat \
  --provider deepseek \
  --model deepseek-chat \
  --system "VocÃª Ã© um assistente de programaÃ§Ã£o" \
  "Escreva uma funÃ§Ã£o para calcular fibonacci"

# Personalizar parÃ¢metros
securellm chat \
  --provider deepseek \
  --model deepseek-coder \
  --max-tokens 2000 \
  --temperature 0.3 \
  "Otimize este cÃ³digo Rust: ..."

# Health check
securellm health deepseek

# Listar modelos disponÃ­veis
securellm models deepseek

# Ver capacidades do provider
securellm info deepseek
```

### API Rust

```rust
use securellm_core::*;
use securellm_providers::deepseek::{DeepSeekConfig, DeepSeekProvider};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configurar provider
    let config = DeepSeekConfig::new(std::env::var("SECURELLM_API_KEY")?)
        .with_timeout(Duration::from_secs(60))
        .with_logging(true);
    
    let provider = DeepSeekProvider::new(config)?;
    
    // Criar requisiÃ§Ã£o
    let request = Request::new("deepseek", "deepseek-chat")
        .with_system("VocÃª Ã© um assistente Ãºtil")
        .add_message(Message {
            role: MessageRole::User,
            content: MessageContent::Text("O que Ã© Rust?".to_string()),
            name: None,
            metadata: None,
        })
        .with_max_tokens(500)
        .mark_sensitive(); // Para dados sensÃ­veis
    
    // Enviar requisiÃ§Ã£o
    let response = provider.send_request(request).await?;
    
    // Processar resposta
    println!("Resposta: {}", response.text()?);
    println!("Tokens usados: {}", response.usage.total_tokens);
    
    Ok(())
}
```

## ğŸ”’ SeguranÃ§a

### NÃ­veis de SeguranÃ§a

```rust
// ConfiguraÃ§Ã£o em config.toml
[security]
security_level = "Critical"  # Low, Medium, High, Critical

# Critical: TLS mÃºtuo, criptografia end-to-end, audit completo
# High: TLS, rate limiting, audit
# Medium: TLS opcional, basic audit
# Low: Apenas para desenvolvimento
```

### TLS MÃºtuo (Production)

```toml
[security]
tls_enabled = true
security_level = "Critical"

[tls]
ca_cert = "/path/to/ca.pem"
client_cert = "/path/to/client.pem"
client_key = "/path/to/client-key.pem"
verify_peer = true
```

### GestÃ£o de Secrets

```bash
# Nunca hardcode API keys!

# MÃ©todo 1: VariÃ¡vel de ambiente
export SECURELLM_API_KEY="your-key"

# MÃ©todo 2: Arquivo de config protegido
chmod 600 ~/.config/securellm/secrets.toml

# MÃ©todo 3: Sistema keyring (em desenvolvimento)
```

### ProteÃ§Ã£o de Dados SensÃ­veis

```rust
let request = Request::new("deepseek", "model")
    .mark_sensitive()  // Ativa proteÃ§Ãµes extras
    .with_caller_id("user-123")  // Para audit trail
    .add_message(...);
```

## ğŸ“Š Auditoria e Monitoramento

### ConfiguraÃ§Ã£o de Audit

```toml
[audit]
enabled = true
log_requests = true
log_responses = false  # Apenas se necessÃ¡rio (pode conter dados sensÃ­veis)
retention_days = 90
database_path = "/var/lib/securellm/audit.db"
```

### Eventos de SeguranÃ§a

O sistema automaticamente loga:
- Falhas de autenticaÃ§Ã£o
- ViolaÃ§Ãµes de rate limit
- Erros TLS
- RequisiÃ§Ãµes com dados sensÃ­veis
- PadrÃµes de acesso incomuns

## ğŸ³ Deploy com Containers

### Docker Compose

```yaml
# containers/docker-compose.yml
version: '3.8'
services:
  securellm:
    build: .
    ports:
      - "8080:8080"
    environment:
      - SECURELLM_API_KEY=${API_KEY}
      - RUST_LOG=info
    volumes:
      - ./config:/config:ro
      - ./data:/data
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
```

### Kubernetes (exemplo)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: securellm
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: securellm
        image: securellm:latest
        securityContext:
          runAsNonRoot: true
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
        env:
        - name: SECURELLM_API_KEY
          valueFrom:
            secretKeyRef:
              name: securellm-secrets
              key: api-key
```

## ğŸ”§ Desenvolvimento

### Setup do Ambiente

```bash
# Com Nix
nix develop

# Sem Nix
cargo build
cargo test
cargo clippy
cargo fmt
```

### Adicionar Novo Provider

1. Criar arquivo em `crates/providers/src/novo_provider.rs`
2. Implementar trait `LLMProvider`
3. Adicionar testes
4. Documentar no README

Exemplo mÃ­nimo:

```rust
use async_trait::async_trait;
use securellm_core::*;

pub struct NovoProvider {
    config: NovoConfig,
}

#[async_trait]
impl LLMProvider for NovoProvider {
    fn name(&self) -> &str { "novo" }
    fn version(&self) -> &str { "v1" }
    
    fn validate_config(&self) -> Result<()> { todo!() }
    async fn send_request(&self, req: Request) -> Result<Response> { todo!() }
    async fn health_check(&self) -> Result<ProviderHealth> { todo!() }
    fn capabilities(&self) -> ProviderCapabilities { todo!() }
    async fn list_models(&self) -> Result<Vec<ModelInfo>> { todo!() }
}
```

## ğŸ“ˆ Roadmap

### âœ… Fase 1: Core Foundation (Atual)
- [x] Estrutura base do projeto
- [x] Provider abstraction layer
- [x] DeepSeek implementation completa
- [x] CLI bÃ¡sico
- [x] Security primitives
- [x] Docker support
- [x] NixOS module

### ğŸš§ Fase 2: Security Hardening (Em andamento)
- [ ] TLS mutual authentication completo
- [ ] Request sandboxing real
- [ ] Rate limiting adaptativo
- [ ] Audit logging com SQLite
- [ ] PII detection e sanitization

### ğŸ“‹ Fase 3: Provider Integration
- [ ] OpenAI adapter
- [ ] Anthropic adapter
- [ ] Ollama (local) adapter
- [ ] llama.cpp integration
- [ ] Custom server support

### ğŸ”® Fase 4: Advanced Features
- [ ] E2E encryption
- [ ] Key rotation automÃ¡tica
- [ ] HSM support
- [ ] Distributed tracing
- [ ] GraphQL API

### ğŸ¨ Fase 5: Distribution
- [ ] Desktop app (Tauri/Iced)
- [ ] Proxy server (Go)
- [ ] Web UI
- [ ] Mobile apps
- [ ] Package managers (apt, yum, brew)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Veja [CONTRIBUTING.md](CONTRIBUTING.md).

### Ãreas que precisam de ajuda:
1. ImplementaÃ§Ã£o de novos providers (OpenAI, Anthropic, Ollama)
2. Testes de integraÃ§Ã£o
3. DocumentaÃ§Ã£o e exemplos
4. Performance optimization
5. Security audits

## ğŸ“ LicenÃ§a

Dual-licensed: MIT OR Apache-2.0

## ğŸ” Reportar Vulnerabilidades

Encontrou uma vulnerabilidade? Reporte privadamente para:
security@securellm.dev

**NÃ£o** crie issues pÃºblicas para vulnerabilidades de seguranÃ§a.

## ğŸ’¬ Suporte

- GitHub Issues: https://github.com/securellm/bridge/issues
- Discussions: https://github.com/securellm/bridge/discussions
- Discord: [em breve]

## ğŸ™ Agradecimentos

ConstruÃ­do com tecnologias incrÃ­veis:
- Rust e Tokio
- rustls para TLS seguro
- Axum para HTTP
- NixOS para builds reproduzÃ­veis

---

**Made with â¤ï¸ for secure AI communication**
