# üöÄ Quick Start - SecureLLM Bridge

## Instala√ß√£o R√°pida

### Op√ß√£o 1: Com Rust (Recomendado)

```bash
# Instalar Rust se n√£o tiver
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

# Clonar e compilar
git clone https://github.com/securellm/bridge.git
cd secure-llm-bridge
make build

# Ou instalar globalmente
make install
```

### Op√ß√£o 2: Com Nix (Para usu√°rios NixOS)

```bash
# Build
nix build

# Rodar direto
nix run . -- --help

# Ou adicionar ao seu sistema
# configuration.nix:
services.securellm = {
  enable = true;
  configFile = /etc/securellm/config.toml;
};
```

### Op√ß√£o 3: Com Docker

```bash
# Build
docker build -t securellm:latest .

# Rodar
docker run --rm \
  -e SECURELLM_API_KEY=your-key \
  securellm:latest \
  chat --provider deepseek --model deepseek-chat "Hello"
```

## Primeiro Uso

### 1. Configure sua API Key

```bash
# DeepSeek API Key (obtenha em https://platform.deepseek.com)
export SECURELLM_API_KEY="your-deepseek-api-key"
```

### 2. Teste a Conex√£o

```bash
# Health check
securellm health deepseek

# Listar modelos
securellm models deepseek
```

### 3. Primeiro Chat

```bash
securellm chat \
  --provider deepseek \
  --model deepseek-chat \
  "Explique o que √© Rust em 3 frases"
```

### 4. Chat com Par√¢metros

```bash
securellm chat \
  --provider deepseek \
  --model deepseek-coder \
  --system "Voc√™ √© um expert em Rust" \
  --max-tokens 2000 \
  --temperature 0.3 \
  "Como implementar um trait customizado?"
```

## Exemplos de Uso

### Chat Simples

```bash
securellm chat -p deepseek -m deepseek-chat "Hello!"
```

### Chat Criativo

```bash
securellm chat \
  -p deepseek \
  -m deepseek-chat \
  --temperature 0.9 \
  "Escreva um poema sobre Rust programming"
```

### Chat para C√≥digo

```bash
securellm chat \
  -p deepseek \
  -m deepseek-coder \
  --system "Expert Rust programmer" \
  "Write a secure HTTP client in Rust"
```

### Usando em Scripts

```bash
#!/bin/bash

RESPONSE=$(securellm chat \
  -p deepseek \
  -m deepseek-chat \
  "What is 2+2?" 2>&1 | grep -A 100 "Response:")

echo "$RESPONSE"
```

## Uso como Biblioteca Rust

### Cargo.toml

```toml
[dependencies]
securellm-core = { path = "./secure-llm-bridge/crates/core" }
securellm-providers = { path = "./secure-llm-bridge/crates/providers" }
tokio = { version = "1", features = ["full"] }
```

### main.rs

```rust
use securellm_core::*;
use securellm_providers::deepseek::{DeepSeekConfig, DeepSeekProvider};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Configurar
    let config = DeepSeekConfig::new(std::env::var("SECURELLM_API_KEY")?);
    let provider = DeepSeekProvider::new(config)?;
    
    // Criar request
    let request = Request::new("deepseek", "deepseek-chat")
        .add_message(Message {
            role: MessageRole::User,
            content: MessageContent::Text("Ol√°!".into()),
            name: None,
            metadata: None,
        });
    
    // Enviar
    let response = provider.send_request(request).await?;
    
    // Usar
    println!("Resposta: {}", response.text()?);
    println!("Tokens: {}", response.usage.total_tokens);
    
    Ok(())
}
```

## Configura√ß√£o Avan√ßada

### Arquivo de Configura√ß√£o

Crie `~/.config/securellm/config.toml`:

```toml
[security]
security_level = "High"
tls_enabled = true

[[providers]]
name = "deepseek"
enabled = true
timeout_seconds = 60

[rate_limiting]
default_limit = 60
```

### Vari√°veis de Ambiente

```bash
export SECURELLM_API_KEY="your-key"
export SECURELLM_CONFIG_PATH="$HOME/.config/securellm/config.toml"
export RUST_LOG=info
```

## Comandos √öteis

```bash
# Ver ajuda completa
securellm --help

# Ver ajuda de um comando
securellm chat --help

# Modo verbose
securellm -v chat -p deepseek -m deepseek-chat "Hello"

# Info do provider
securellm info deepseek

# Listar modelos
securellm models deepseek

# Health check
securellm health deepseek
```

## Troubleshooting

### API Key n√£o encontrada

```bash
# Certifique-se de exportar a vari√°vel
export SECURELLM_API_KEY="your-key"

# Ou passe diretamente
securellm chat --api-key "your-key" -p deepseek -m deepseek-chat "test"
```

### Erro de compila√ß√£o

```bash
# Limpar e recompilar
make clean
make build

# Ou com cargo
cargo clean
cargo build --release
```

### Erro de network

```bash
# Testar conex√£o
curl https://api.deepseek.com/v1/models

# Ver logs detalhados
RUST_LOG=debug securellm -v chat -p deepseek -m deepseek-chat "test"
```

## Pr√≥ximos Passos

1. ‚úÖ Configure o DeepSeek (j√° funcionando!)
2. üîú Aguarde implementa√ß√£o do OpenAI
3. üîú Aguarde implementa√ß√£o do Anthropic
4. üîú Teste o Ollama para modelos locais

## Links √öteis

- **Documenta√ß√£o Completa**: [docs/GETTING_STARTED.md](docs/GETTING_STARTED.md)
- **Seguran√ßa**: [docs/SECURITY.md](docs/SECURITY.md)
- **Exemplos**: [examples/](examples/)
- **API DeepSeek**: https://platform.deepseek.com

## Suporte

- Issues: GitHub Issues
- Discuss√µes: GitHub Discussions
- Email: help@securellm.dev

---

**Aproveite o SecureLLM Bridge! üöÄ**
